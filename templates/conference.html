<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conference</title>
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }
        .container {
            text-align: center;
        }
        .video {
            width: 100%;
            height: auto;
            margin-top: 20px;
        }
        .button-group {
            margin-top: 20px;
        }
        .user-grid {
            margin-top: 20px;
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            grid-gap: 10px;
            justify-content: center;
        }
        .user-grid img {
            max-width: 100px;
            margin: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Welcome, {{ user_name }}</h1>
        <div class="video">
            <!-- Webcam view of the user -->
            <video id="webcam" autoplay playsinline style="width: 100%;"></video>
        </div>
        <div class="button-group">
            <button class="btn btn-primary" id="micButton">Mic On</button>
            <button class="btn btn-primary" id="cameraButton">Camera On</button>
            <a href="end" class="btn btn-danger">End</a>
        </div>
        <div class="user-grid">
            <!-- Grid view of other users (default images) -->
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
            <img src="https://via.placeholder.com/100" alt="User">
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands"></script>
    <script>
        const video = document.getElementById('webcam');
        const micButton = document.getElementById('micButton');
        const cameraButton = document.getElementById('cameraButton');

        let isMicOn = true;
        let isCameraOn = true;

        micButton.addEventListener('click', () => {
            isMicOn = !isMicOn;
            micButton.textContent = isMicOn ? 'Mic On' : 'Mic Off';
            const tracks = video.srcObject.getAudioTracks();
            tracks[0].enabled = isMicOn;
        });

        cameraButton.addEventListener('click', () => {
            isCameraOn = !isCameraOn;
            cameraButton.textContent = isCameraOn ? 'Camera On' : 'Camera Off';
            const tracks = video.srcObject.getVideoTracks();
            tracks[0].enabled = isCameraOn;
        });

        // Initialize face-api.js
        Promise.all([
            faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
            faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
            faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
            faceapi.nets.faceExpressionNet.loadFromUri('/models')
        ]).then(startVideo);

        function startVideo() {
            navigator.getUserMedia(
                { video: {} },
                stream => {
                    video.srcObject = stream;
                    video.addEventListener('play', () => {
                        const canvas = faceapi.createCanvasFromMedia(video);
                        document.body.append(canvas);
                        const displaySize = { width: video.width, height: video.height };
                        faceapi.matchDimensions(canvas, displaySize);
                        setInterval(async () => {
                            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptors().withFaceExpressions();
                            const resizedDetections = faceapi.resizeResults(detections, displaySize);
                            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                            faceapi.draw.drawDetections(canvas, resizedDetections);
                            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
                            faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
                        }, 100);
                    });
                },
                err => console.error(err)
            )
        }

        // Initialize MediaPipe Hands
        const hands = new Hands({locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
        }});
        hands.setOptions({
            maxNumHands: 1,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        hands.onResults((results) => {
            // Process hand tracking results
            console.log(results);
        });
        hands.start();
    </script>
</body>
</html>
